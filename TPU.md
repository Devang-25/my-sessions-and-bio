
## Title

Cloud TPU and Cloud TPU Pod: AI supercomputing for large scale machine learning

## Title (JP)

Tensor Processing Unit (TPU)とTPU Pod：機械学習のためのAIスーパーコンピュータ

## Session duration

about 45 min

## Blog post

This session is based on the content of blog post: [What makes TPUs fine-tuned for deep learning?](https://cloud.google.com/blog/products/ai-machine-learning/what-makes-tpus-fine-tuned-for-deep-learning) and [An in-depth look at Google’s first Tensor Processing Unit (TPU)](https://cloud.google.com/blog/big-data/2017/05/an-in-depth-look-at-googles-first-tensor-processing-unit-tpu)

## Target audience

Data engineers and analysts, hardware developers

## Short agenda (less than 400 chars)
Tensor Processing Unit (TPU) is a LSI designed by Google for neural network processing. TPU Pod is a large scale computing cluster that consolidates up to 1024 cores of Cloud TPU v2. In this session, we will learn the technical details of TPU and TPU Pod, and AI supercomputing environment enables a large scale model parallelism for deep learning training.

## Long agenda: 
Cloud Tensor Processing Unit (TPU) is a LSI designed by Google for neural network processing. TPU features a domain specific architecture that is designed specifically for accelerating TensorFlow training and prediction workloads, provides significant performance benefit on machine learning production use. Cloud TPU Pod is a large scale computing cluster that consolidates up to 1024 cores of Cloud TPU, interconnected with Google's high speed interconnect. In this session, we will learn the technical details of Cloud TPU and Cloud TPU Pod, and new features of TensorFlow that enables a large scale model parallelism for deep learning training.

## Long agenda


## Abstract (JP):

このセッションでは、Googleが開発したニューラルネットワーク演算専用LSI「Tensor Processing Unit (TPU)」を紹介します。ニューラルネットワークモデルのサイズを大幅に縮小する「量子化」の技術をはじめ、CISC設計、TPUの心臓部にあたるシストリックアレイの設計について解説。ニューラルネットワークの推論に特化したミニマルな設計によって、CPUやGPUを大幅に上回る性能のLSIをわずか15か月で実現したテクノロジーに焦点を当てます。
